{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Pensieve™\n",
    "An Enhanced Deep Residual (EDSR) Information Maximizing Variational Auto-Encoder (InfoVAE) with Group Normalization (GN), Residual Bottleneck Attention Modules (RBAM), Efficient Sub-Pixel Convolution Super-Resolution (ESPCN), and Perceptual Similarity Loss (SSIM)\n",
    "\n",
    "<table><tr>\n",
    "<td><img src='https://s3.amazonaws.com/neurokinetikz/latent-animation-1540551741.4923084-final.gif'></td>\n",
    "<td><img src='https://s3.amazonaws.com/neurokinetikz/latent-animation-1540552110.470284-final.gif'></td>\n",
    "<td><img src=\"https://s3.amazonaws.com/neurokinetikz/1542113809.6163912-080.gif\"></td>\n",
    "<td><img src='https://s3.amazonaws.com/neurokinetikz/latent-animation-1540552122.395882-final.gif'></td>\n",
    "<td><img src='https://s3.amazonaws.com/neurokinetikz/latent-animation-1540551578.5925505-final.gif'></td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Stage Variational Auto-Encoders for Coarse-to-Fine Image Generation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1705.07202\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/neurokinetikz/download-1.png\">\n",
    "\n",
    "Variational auto-encoder (VAE) is a powerful unsupervised learning framework for image generation. One drawback of VAE is that it generates blurry images due to its Gaussianity assumption and thus L2 loss. To allow the generation of high quality images by VAE, we increase the capacity of decoder network by employing residual blocks and skip connections, which also enable efficient optimization. To overcome the limitation of L2 loss, we propose to generate images in a multi-stage manner from coarse to fine. In the simplest case, the proposed multi-stage VAE divides the decoder into two components in which the second component generates refined images based on the course images generated by the first component. Since the second component is independent of the VAE model, it can employ other loss functions beyond the L2 loss and different model architectures. The proposed framework can be easily generalized to contain more than two components. Experiment results on the MNIST and CelebA datasets demonstrate that the proposed multi-stage VAE can generate sharper images as compared to those from the original VAE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from libs import utils, gif\n",
    "from libs.group_norm import GroupNormalization\n",
    "from libs.variance_pooling import GlobalVariancePooling2D\n",
    "\n",
    "from keras.models import Model, load_model, model_from_json\n",
    "from keras.layers import Input, Flatten, Reshape, Add, Multiply, Activation, Lambda\n",
    "from keras.layers import Dense, Conv2D, DepthwiseConv2D, SeparableConv2D\n",
    "from keras.layers import MaxPooling2D, UpSampling2D, GlobalAveragePooling2D\n",
    "from keras.callbacks import LambdaCallback\n",
    "\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "\n",
    "from keras_contrib.losses import DSSIMObjective\n",
    "from keras_contrib.layers.convolutional import SubPixelUpscaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DIRECTORY = 'roadtrip'\n",
    "\n",
    "SIZE = 256\n",
    "CHANNELS = 3\n",
    "\n",
    "SCALE_FACTOR = 2\n",
    "\n",
    "FEATURES = SIZE*SIZE*CHANNELS\n",
    "FEATURES_2X = SCALE_FACTOR*SIZE*SCALE_FACTOR*SIZE*CHANNELS\n",
    "\n",
    "MODEL_NAME = DIRECTORY+'-'+str(SIZE)+'-'+str(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images:\t184\n",
      "Loading images:\t184\n",
      "MODEL:  roadtrip-256-1545488800.133229\n",
      "IMGS:  (184, 256, 256, 3) (184, 512, 512, 3)\n",
      "FLAT:  (184, 196608) (184, 786432)\n",
      "SAMPLES:  (9, 196608) (9, 786432)\n"
     ]
    }
   ],
   "source": [
    "# load images\n",
    "imgs, xs, ys  = utils.load_images(directory=\"imgs/\"+DIRECTORY,rx=SIZE,ry=SIZE)\n",
    "imgs_2x, xs_2x, ys_2x = utils.load_images(directory=\"imgs/\"+DIRECTORY,rx=SCALE_FACTOR*SIZE,ry=SCALE_FACTOR*SIZE)\n",
    "\n",
    "# normalize pixels\n",
    "IMGS = imgs/127.5 - 1\n",
    "FLAT = np.reshape(IMGS,(-1,FEATURES))\n",
    "\n",
    "IMGS_2X = imgs_2x/127.5 - 1\n",
    "FLAT_2X = np.reshape(IMGS_2X,(-1,FEATURES_2X)) \n",
    "\n",
    "SAMPLES =  np.random.permutation(FLAT)[:9]\n",
    "SAMPLES_2X =  np.random.permutation(FLAT_2X)[:9]\n",
    "\n",
    "TOTAL_BATCH = IMGS.shape[0]\n",
    "\n",
    "# print shapes\n",
    "print(\"MODEL: \",MODEL_NAME)\n",
    "print(\"IMGS: \",IMGS.shape,IMGS_2X.shape)\n",
    "print(\"FLAT: \",FLAT.shape,FLAT_2X.shape)\n",
    "print(\"SAMPLES: \",SAMPLES.shape,SAMPLES_2X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Very Deep Convolutional Networks for Large-Scale Image Recognition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1409.1556\n",
    "\n",
    "<img src=\"https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_3/CascadingConvolutions.png\">\n",
    "\n",
    "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3×3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16–19 weight layers.\n",
    "\n",
    "First, we incorporate three non-linear rectification layers instead of a single one, which makes the decision function more discriminative.\n",
    "\n",
    "Second, we decrease the number of parameters: assuming that both the input and the output of a\n",
    "three-layer 3 × 3 convolution stack has C channels, the stack is parametrised by (W) weights; at the same time, a single 7 × 7 conv. layer would require 81% more. This can be seen as imposing a regularisation on the 7 × 7 conv. filters, forcing them to have a decomposition through the 3 × 3 filters (with non-linearity injected in between)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/syncedreview/facebook-ai-proposes-group-normalization-alternative-to-batch-normalization-fb0699bffae7\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/neurokinetikz/groupnorm.png\">\n",
    "\n",
    "\n",
    "The mainstream normalization technique for almost all convolutional neural networks today is Batch Normalization (BN), which has been widely adopted in the development of deep learning. Proposed by Google in 2015, BN can not only accelerate a model’s converging speed, but also alleviate problems such as Gradient Dispersion in the deep neural network, making it easier to train models.\n",
    "\n",
    "Dr. Wu and Dr. He however argue in their paper Group Normalization that normalizing with batch size has limitations, as BN cannot ensure the model accuracy rate when the batch size becomes smaller. As a result, researchers today are normalizing with large batches, which is very memory intensive, and are avoiding using limited memory to explore higher-capacity models.\n",
    "\n",
    "Dr. Wu and Dr. He believe their new GN technique is a simple but effective alternative to BN. Specifically, GN divides channels — also referred to as feature maps that look like 3D chunks of data — into groups and normalizes the features within each group. GN only exploits the layer dimensions, and its computation is independent of batch sizes.\n",
    "\n",
    "The paper reports that GN had a 10.6% lower error rate than its BN counterpart for ResNet-50 in ImageNet with a batch size of 2 samples; and matched BN performance while outperforming other normalization techniques with a regular batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"800\" src=\"https://s3.amazonaws.com/neurokinetikz/Asset+4.png\">\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode(x):\n",
    "    # set current layer\n",
    "    current_layer = Reshape((SIZE,SIZE,CHANNELS))(x)\n",
    "    \n",
    "    # convolution layers\n",
    "    for layer, n_filters in enumerate(FILTERS):\n",
    "\n",
    "        # stacked 3x3 convolutions with group normalization + activation\n",
    "        current_layer = Conv2D(n_filters,3,padding='SAME',kernel_initializer=INITIALIZER)(current_layer)\n",
    "        current_layer = GroupNormalization(groups=n_filters,axis=-1)(current_layer)\n",
    "        current_layer = Activation(ACTIVATION)(current_layer)\n",
    "\n",
    "        current_layer = Conv2D(n_filters,3,padding='SAME',kernel_initializer=INITIALIZER)(current_layer)\n",
    "        current_layer = GroupNormalization(groups=n_filters,axis=-1)(current_layer)\n",
    "        current_layer = Activation(ACTIVATION)(current_layer)\n",
    "         \n",
    "        # max pooling\n",
    "        current_layer = MaxPooling2D()(current_layer)\n",
    "    \n",
    "    # grab the last shape for reconstruction\n",
    "    shape = current_layer.get_shape().as_list()\n",
    "    \n",
    "    # flatten\n",
    "    flat = Flatten()(current_layer)\n",
    "    \n",
    "    # latent vector\n",
    "    z = Dense(LATENT_DIM,name='encoder')(flat)\n",
    "    \n",
    "    return z, (shape[1],shape[2],shape[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InfoVAE: Information Maximizing Variational Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1706.02262v3\n",
    "\n",
    "https://ermongroup.github.io/blog/a-tutorial-on-mmd-variational-autoencoders/\n",
    "\n",
    "<table><tr>\n",
    "<td><img src=\"https://s3.amazonaws.com/neurokinetikz/kl_latent.gif\" ></td>\n",
    "<td><img src=\"https://s3.amazonaws.com/neurokinetikz/mmd_latent.gif\" ></td>\n",
    "</tr></table>\n",
    "\n",
    "Maximum mean discrepancy (MMD, (Gretton et al. 2007)) is based on the idea that two distributions are identical if and only if all their moments are the same. Therefore, we can define a divergence by measuring how “different” the moments of two distributions p(z) and q(z) are. MMD can accomplish this efficiently via the kernel embedding trick:\n",
    "\n",
    "A kernel can be intuitively interpreted as a function that measures the “similarity” of two samples. It has a large value when two samples are similar, and small when they are different. For example, the Gaussian kernel considers points that are close in Euclidean space to be “similar”. A rough intuition of MMD, then, is that if two distributions are identical, then the average “similarity” between samples from each distribution, should be identical to the average “similarity” between mixed samples from both distributions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_kernel(x, y):\n",
    "    x_size = tf.shape(x)[0]\n",
    "    y_size = tf.shape(y)[0]\n",
    "    dim = tf.shape(x)[1]\n",
    "    tiled_x = tf.tile(tf.reshape(x, tf.stack([x_size, 1, dim])), tf.stack([1, y_size, 1]))\n",
    "    tiled_y = tf.tile(tf.reshape(y, tf.stack([1, y_size, dim])), tf.stack([x_size, 1, 1]))\n",
    "    return tf.exp(-tf.reduce_mean(tf.square(tiled_x - tiled_y), axis=2) / tf.cast(dim, tf.float32))\n",
    "\n",
    "def compute_mmd(x, y):\n",
    "    x_kernel = compute_kernel(x, x)\n",
    "    y_kernel = compute_kernel(y, y)\n",
    "    xy_kernel = compute_kernel(x, y)\n",
    "    return tf.reduce_mean(x_kernel) + tf.reduce_mean(y_kernel) - 2 * tf.reduce_mean(xy_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vae_loss(y_true,y_pred):\n",
    "    epsilon = tf.random_normal(tf.stack([BATCH_SIZE, LATENT_DIM]))\n",
    "    latent_loss = compute_mmd(epsilon, y_pred)\n",
    "    return latent_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"800\" src=\"https://s3.amazonaws.com/neurokinetikz/Asset+5.png\">\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode(z,z_g,shape=None):\n",
    "    \n",
    "    # reverse the encoder\n",
    "    filters = FILTERS[::-1]\n",
    "\n",
    "    # inflate\n",
    "    inflated = shape[0]*shape[1]*shape[2]\n",
    "    inflate = Dense(inflated,name='generator')\n",
    "    current_layer = inflate(z) ; generator = inflate(z_g)\n",
    "    \n",
    "    # reshape\n",
    "    reshape = Reshape(shape)\n",
    "    current_layer = reshape(current_layer) ; generator = reshape(generator)\n",
    "    \n",
    "    # build layers\n",
    "    for layer, n_filters in enumerate(filters):\n",
    "        \n",
    "        # upsample\n",
    "        u = UpSampling2D()\n",
    "        current_layer = u(current_layer) ; generator = u(generator)\n",
    "\n",
    "        # stacked 3x3 convolutions with group normalization + activation\n",
    "        c1 = Conv2D(n_filters,3,padding='SAME',kernel_initializer=INITIALIZER)\n",
    "        b1 = GroupNormalization(groups=n_filters,axis=-1)\n",
    "        a1 = Activation(ACTIVATION)\n",
    "\n",
    "        current_layer = c1(current_layer) ; generator = c1(generator)\n",
    "        current_layer = b1(current_layer) ; generator = b1(generator)\n",
    "        current_layer = a1(current_layer) ; generator = a1(generator)\n",
    "\n",
    "        c2 = Conv2D(n_filters,3,padding='SAME',kernel_initializer=INITIALIZER)\n",
    "        b2 = GroupNormalization(groups=n_filters,axis=-1)\n",
    "        a2 = Activation(ACTIVATION)\n",
    "\n",
    "        current_layer = c2(current_layer) ; generator = c2(generator)\n",
    "        current_layer = b2(current_layer) ; generator = b2(generator)\n",
    "        current_layer = a2(current_layer) ; generator = a2(generator)\n",
    "    \n",
    "    # output convolution + activation\n",
    "    conv = Conv2D(CHANNELS,1,padding='SAME')\n",
    "    activation = Activation('tanh',name='decoder_dssim')\n",
    "    \n",
    "    current_layer = conv(current_layer)       ; generator = conv(generator)\n",
    "    current_layer = activation(current_layer) ; generator = activation(generator)\n",
    "    \n",
    "    flatten = Flatten(name='decoder')\n",
    "    decoder_loss = flatten(current_layer)\n",
    "    \n",
    "    return current_layer, generator, decoder_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deconvolution and Checkerboard Artifacts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://distill.pub/2016/deconv-checkerboard/\n",
    "\n",
    "When we have neural networks generate images, we often have them build them up from low resolution, high-level descriptions. This allows the network to describe the rough image and then fill in the details.\n",
    "\n",
    "In order to do this, we need some way to go from a lower resolution image to a higher one. We generally do this with the deconvolution operation. Roughly, deconvolution layers allow the model to use every point in the small image to “paint” a square in the larger one.\n",
    "\n",
    "Unfortunately, deconvolution can easily have “uneven overlap,” putting more of the metaphorical paint in some places than others. In particular, deconvolution has uneven overlap when the kernel size (the output window size) is not divisible by the stride (the spacing between points on the top). While the network could, in principle, carefully learn weights to avoid this  — as we’ll discuss in more detail later — in practice neural networks struggle to avoid it completely.\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/neurokinetikz/download-2.png\">\n",
    "\n",
    "To avoid these artifacts, we’d like an alternative to regular deconvolution (“transposed convolution”). Unlike deconvolution, this approach to upsampling shouldn’t have artifacts as its default behavior. Ideally, it would go further, and be biased against such artifacts.\n",
    "\n",
    "One approach is to separate out upsampling to a higher resolution from convolution to compute features. For example, you might resize the image (using nearest-neighbor interpolation or bilinear interpolation) and then do a convolutional layer. This seems like a natural approach, and roughly similar methods have worked well in image super-resolution.\n",
    "\n",
    "Our experience has been that nearest-neighbor resize followed by a convolution works very well, in a wide variety of contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refiner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"https://s3.amazonaws.com/neurokinetikz/Asset+7.png\">\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def refine(x,x_g,name='refiner'):\n",
    "    # 1x1 channel convolution\n",
    "    c1 = Conv2D(CHANNELS,1,padding='SAME')\n",
    "    current_layer = c1(x) ; generator = c1(x_g)\n",
    "    \n",
    "    # shortcut\n",
    "    shortcut = current_layer; shortcut_g = generator\n",
    "    \n",
    "    # reshape convolution\n",
    "    c2 = Conv2D(R_FILTERS,3,padding='SAME',kernel_initializer=INITIALIZER)\n",
    "    current_layer = c2(x) ; generator = c2(x_g)\n",
    "\n",
    "    # residual layers\n",
    "    for i in range(R_LAYERS):\n",
    "        current_layer, generator = residual(current_layer, generator, R_ATTENTION)\n",
    "    \n",
    "    # output convolution\n",
    "    c3 = Conv2D(CHANNELS,1,padding='SAME')\n",
    "    current_layer = c3(current_layer); generator = c3(generator)\n",
    "    \n",
    "    # merge shortcut\n",
    "    merge = Add()\n",
    "    current_layer = merge([current_layer, shortcut]) ; generator = merge([generator, shortcut_g])\n",
    "    \n",
    "    #activate\n",
    "    activation = Activation('tanh',name=name+'_dssim')\n",
    "    current_layer = activation(current_layer) ; generator = activation(generator)\n",
    "    \n",
    "    #flatten\n",
    "    flatten = Flatten(name=name)\n",
    "    refiner_loss = flatten(current_layer)\n",
    "    \n",
    "    return current_layer, generator, refiner_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enhanced Deep Residual Networks for Single Image Super-Resolution (EDSR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1707.02921\n",
    "\n",
    "<img width=500 src=\"https://s3.amazonaws.com/neurokinetikz/Screen+Shot+2018-11-18+at+8.54.43+AM.png\">\n",
    "\n",
    "Recently, the powerful capability of deep neural networks has led to dramatic improvements in SR. Since Dong et al. [4, 5] first proposed a deep learning-based SR method, various CNN architectures have been studied for SR. Kim et al. [11, 12] first introduced the residual network for training much deeper network architectures and achieved superior performance. In particular, they showed that skip connection and recursive convolution alleviate the burden of carrying identity information in the super-resolution network. Similarly to [20], Mao et al. [16] tackled the general image restoration problem with encoder-decoder networks and symmetric skip connections. In [16], they argue that those nested skip connections provide fast and improved convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1707.02921\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/neurokinetikz/download-3.png\">\n",
    "\n",
    "Recently, residual networks exhibit excellent performance in computer vision problems from the lowlevel to high-level tasks. Although Ledig et al. successfully applied the ResNet architecture to the super-resolution problem with SRResNet, we further improve the performance by employing better ResNet structure.\n",
    "\n",
    "We remove the batch normalization layers from our network as Nah et al.[19] presented in their image deblurring work. Since batch normalization layers normalize the features, they get rid of range flexibility from networks by normalizing the features, it is better to remove them. We experimentally show that this simple modification increases the performance substantially as detailed in\n",
    "\n",
    "Furthermore, GPU memory usage is also sufficiently reduced since the batch normalization layers consume the same amount of memory as the preceding convolutional layers. Our baseline model without batch normalization layer saves approximately 40% of memory usage during training, compared to SRResNet. Consequently, we can build up a larger model that has better performance than conventional ResNet structure under limited computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def residual(x,x_g,attention=False):\n",
    "    # current layer\n",
    "    current_layer = x ; generator = x_g\n",
    "\n",
    "    # shortcuts\n",
    "    shortcut = current_layer ; shortcut_g = generator\n",
    "\n",
    "    # conv 1\n",
    "    c1 = Conv2D(R_FILTERS,3,padding='SAME',kernel_initializer=INITIALIZER)\n",
    "    current_layer = c1(current_layer) ; generator = c1(generator)\n",
    "    \n",
    "    # activation 1\n",
    "    a1 = Activation(ACTIVATION)\n",
    "    current_layer = a1(current_layer) ; generator = a1(generator)\n",
    "\n",
    "    # conv 2\n",
    "    c2 = Conv2D(R_FILTERS,3,padding='SAME',kernel_initializer=INITIALIZER)\n",
    "    current_layer = c2(current_layer) ; generator = c2(generator)\n",
    "    \n",
    "    # residual scaling\n",
    "    scale = Lambda(lambda x: x * R_SCALING)\n",
    "    current_layer = scale(current_layer) ; generator = scale(generator)\n",
    "    \n",
    "    # residual attention\n",
    "    if(attention):\n",
    "        current_layer, generator = residual_attention(current_layer,generator)\n",
    "    \n",
    "    # merge shortcut\n",
    "    merge = Add()\n",
    "    current_layer = merge([current_layer, shortcut]) ; generator = merge([generator, shortcut_g])\n",
    "\n",
    "    return current_layer, generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width='800' src=\"https://s3.amazonaws.com/neurokinetikz/Screen+Shot+2018-12-15+at+5.14.32+PM.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Super-Resolution Using Very Deep Residual Channel Attention Networks (RCAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1807.02758v2\n",
    "\n",
    "<img width=\"600\" src=\"https://s3.amazonaws.com/neurokinetikz/Screen+Shot+2018-12-16+at+11.12.08+AM.png\">\n",
    "\n",
    "To make a further step, we propose channel attention (CA) mechanism to adaptively rescale each channel-wise feature by modeling the interdependencies across feature channels. Such CA mechanism allows our proposed network to concentrate on more useful channels and enhance discriminative learning ability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Attention Module for Single Image Super-Resolution (RAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1811.12043\n",
    "\n",
    "<img width=\"500\" src=\"https://s3.amazonaws.com/neurokinetikz/Screen+Shot+2018-12-07+at+8.51.39+AM.png\">\n",
    "\n",
    "In this paper, we propose a new attention method, which is composed of new channel-wise and spatial attention mechanisms optimized for SR and a new fused attention to combine them. Based on this, we propose a new residual attention module (RAM) and a SR network using RAM (SRRAM). We provide in-depth experimental analysis of different attention mechanisms in SR. It is shown that the proposed method can construct both deep and lightweight SR networks showing improved performance in comparison to existing state-of-the-art methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BAM: Bottleneck Attention Module "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1807.06514v2\n",
    "\n",
    "<img width=\"600\" src=\"https://s3.amazonaws.com/neurokinetikz/Screen+Shot+2018-12-16+at+10.15.09+AM.png\">\n",
    "\n",
    "In this work, we focus on the effect of attention in general deep neural networks. We propose a simple and effective attention module, named Bottleneck Attention Module (BAM), that can be integrated with any feed-forward convolutional neural networks. Our module infers an attention map along two separate pathways, channel and spatial. We place our module at each bottleneck of models where the downsampling of feature maps occurs. Our module constructs a hierarchical attention at bottlenecks with a number of parameters and it is trainable in an end-to-end manner jointly with any feed-forward models.\n",
    "\n",
    "As the channels of feature maps can be regarded as feature detectors, the two branches (spatial and channel) explicitly learn ‘what’ and ‘where’ to focus on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s3.amazonaws.com/neurokinetikz/Asset+8.png\">\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def residual_attention(x,x_g):\n",
    "    # current layer\n",
    "    current_layer = x ; generator = x_g\n",
    "    \n",
    "    # shortcuts\n",
    "    shortcut = current_layer ; shortcut_g = generator\n",
    "    \n",
    "    # channel attention\n",
    "    ca, ca_g = channel_attention(current_layer, generator)\n",
    "    \n",
    "    # spatial attention\n",
    "    sa, sa_g = spatial_attention(current_layer, generator)\n",
    "    \n",
    "    # fuse channel and spatial attention\n",
    "    fuse = Add()\n",
    "    current_layer = fuse([ca,sa]); generator = fuse([ca_g,sa_g])\n",
    "    \n",
    "    # sigmoid activation\n",
    "    s = Activation(\"sigmoid\")\n",
    "    current_layer = s(current_layer) ; generator = s(generator)\n",
    "    \n",
    "    # merge fused attention with shortcut\n",
    "    m = Multiply()\n",
    "    current_layer = m([current_layer,shortcut]) ; generator = m([generator, shortcut_g])\n",
    "    \n",
    "    return current_layer, generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def channel_attention(x,x_g):\n",
    "    # current layer\n",
    "    current_layer = x ; generator = x_g\n",
    "    \n",
    "    # global variance pooling\n",
    "    gvp = GlobalVariancePooling2D(); reshape = Reshape((1,1,R_FILTERS))\n",
    "    current_layer = reshape(gvp(current_layer)); generator = reshape(gvp(generator))\n",
    "    \n",
    "    # squeeze\n",
    "    squeeze = Conv2D(int(R_FILTERS/R_REDUCTION),1,padding='SAME',kernel_initializer=INITIALIZER)\n",
    "    current_layer = squeeze(current_layer); generator = squeeze(generator);\n",
    "    \n",
    "    # excitation\n",
    "    a1 = Activation(ACTIVATION)\n",
    "    current_layer = a1(current_layer); generator = a1(generator)\n",
    "    \n",
    "    # scaling\n",
    "    c2 = Conv2D(R_FILTERS,1,padding='SAME',kernel_initializer=INITIALIZER)\n",
    "    current_layer = c2(current_layer) ; generator = c2(generator)\n",
    "        \n",
    "    return current_layer, generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spatial_attention(x,x_g):\n",
    "    # current layer\n",
    "    current_layer = x ; generator = x_g\n",
    "    \n",
    "    # 1x1 convolution\n",
    "    c1 = Conv2D(int(R_FILTERS/R_REDUCTION),1,padding='SAME',kernel_initializer=INITIALIZER)\n",
    "    current_layer = c1(current_layer); generator = c1(generator)\n",
    "\n",
    "    # dilated convolution\n",
    "    c2 = Conv2D(int(R_FILTERS/R_REDUCTION),3,dilation_rate=R_DILATION,padding='SAME',kernel_initializer=INITIALIZER)\n",
    "    current_layer = c2(current_layer) ; generator = c2(generator)\n",
    "\n",
    "    # dilated convolution\n",
    "    c3 = Conv2D(int(R_FILTERS/R_REDUCTION),3,dilation_rate=R_DILATION,padding='SAME',kernel_initializer=INITIALIZER)\n",
    "    current_layer = c3(current_layer) ; generator = c3(generator)\n",
    "\n",
    "    # 1x1 convolution\n",
    "    c4 = Conv2D(1,1,padding='SAME',kernel_initializer=INITIALIZER)\n",
    "    current_layer = c4(current_layer); generator = c4(generator)\n",
    "    \n",
    "    # group normalization\n",
    "    gn = GroupNormalization(groups=1,axis=-1)\n",
    "    current_layer = gn(current_layer); generator = gn(generator)\n",
    "    \n",
    "    return current_layer, generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1609.05158\n",
    "\n",
    "<img width=\"75%\" src=\"https://s3.amazonaws.com/neurokinetikz/Screen+Shot+2018-11-23+at+10.26.45+AM.png\">\n",
    "\n",
    "Recently, several models based on deep neural networks have achieved great success in terms of both reconstruction accuracy and computational performance for single image super-resolution. In these methods, the low resolution (LR) input image is upscaled to the high resolution (HR) space using a single filter, commonly bicubic interpolation, before reconstruction. This means that the super-resolution (SR) operation is performed in HR space. We demonstrate that this is sub-optimal and adds computational complexity. \n",
    "\n",
    "In this paper, we present the first convolutional neural network (CNN) capable of real-time SR of 1080p videos on a single K2 GPU. To achieve this, we propose a novel CNN architecture where the feature maps are extracted in the LR space. In addition, we introduce an efficient sub-pixel convolution layer which learns an array of upscaling filters to upscale the final LR feature maps into the HR output. By doing so, we effectively replace the handcrafted bicubic filter in the SR pipeline with more complex upscaling filters specifically trained for each feature map, whilst also reducing the computational complexity of the overall SR operation. \n",
    "\n",
    "We evaluate the proposed approach using images and videos from publicly available datasets and show that it performs significantly better (+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster than previous CNN-based methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def upsample(x,x_g,img_g,name='super'):\n",
    "    \n",
    "    # current layer\n",
    "    current_layer = x ; generator = x_g; img_generator = img_g\n",
    "    \n",
    "    # convolution\n",
    "    c1 = Conv2D(R_FILTERS*SCALE_FACTOR*SCALE_FACTOR,3,padding='SAME',kernel_initializer=INITIALIZER)\n",
    "    current_layer = c1(current_layer) ; generator = c1(generator) ; img_generator = c1(img_generator)\n",
    "    \n",
    "    # activation\n",
    "    a1 = Activation(ACTIVATION)\n",
    "    current_layer = a1(current_layer) ; generator = a1(generator); img_generator = a1(img_generator)\n",
    "    \n",
    "    # sub-pixel upscaling\n",
    "    upscale = SubPixelUpscaling(scale_factor=SCALE_FACTOR)\n",
    "    current_layer = upscale(current_layer); generator = upscale(generator); img_generator = upscale(img_generator)\n",
    "    \n",
    "    # In practice, it is useful to have a second convolution layer after the \n",
    "    # SubPixelUpscaling layer to speed up the learning process.\n",
    "    c2 = Conv2D(R_FILTERS*SCALE_FACTOR*SCALE_FACTOR,3,padding='SAME',kernel_initializer=INITIALIZER)\n",
    "    current_layer = c2(current_layer) ; generator = c2(generator); img_generator = c2(img_generator)\n",
    "    \n",
    "    # activation\n",
    "    a2 = Activation(ACTIVATION)\n",
    "    current_layer = a2(current_layer) ; generator = a2(generator); img_generator = a2(img_generator)\n",
    "    \n",
    "    # convolution\n",
    "    c3 = Conv2D(CHANNELS,1,padding='SAME')\n",
    "    current_layer = c3(current_layer); generator = c3(generator); img_generator = c3(img_generator)\n",
    "    \n",
    "    # activation\n",
    "    a3 = Activation('tanh',name=name+\"_dssim\")\n",
    "    current_layer = a3(current_layer) ; generator = a3(generator); img_generator = a3(img_generator)\n",
    "    \n",
    "    # flatten\n",
    "    flatten = Flatten(name=name)\n",
    "    upscale_loss = flatten(current_layer)\n",
    "    \n",
    "    return current_layer, generator, img_generator, upscale_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1511.07289\n",
    "\n",
    "<img width=\"300\" style=\"float:left;\" src=\"https://blogs.mathworks.com/deep-learning/files/2017/12/defining_elu_layer_01.png\">\n",
    "\n",
    "We introduce the \"exponential linear unit\" (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PReLUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However, ELUs have improved learning characteristics compared to the units with other activation functions. \n",
    "\n",
    "In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. \n",
    "\n",
    "Therefore, ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. In experiments, ELUs lead not only to faster learning, but also to significantly better generalization performance than ReLUs and LReLUs on networks with more than 5 layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# default activation\n",
    "ACTIVATION  = 'elu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters in Action! Part II — Weight Initializers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/hyper-parameters-in-action-part-ii-weight-initializers-35aee1a28404\n",
    "\n",
    "<img width=\"300\" style=\"float:left;\" src=\"https://cdn-images-1.medium.com/max/1600/1*WLUL_bcjsNK9sXNw6nC-cg.png\">\n",
    "\n",
    "If you dug a little bit deeper, you’ve likely also found out that one should use Xavier / Glorot initialization if the activation function is a Tanh, and that He initialization is the recommended one if the activation function is a ReLU.\n",
    "\n",
    "In summary, for a ReLU activated network, the He initialization scheme using an Uniform distribution is a pretty good choice.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initializers\n",
    "INITIALIZER = 'he_uniform'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning to Generate Images with Perceptual Similarity Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1511.06409\n",
    "\n",
    "<img width=\"200\" style=\"float:left;\" src=\"https://s3.amazonaws.com/neurokinetikz/download-4.png\">\n",
    "\n",
    "In this paper, we explore loss functions that, unlike MSE, MAE, and likelihoods, are grounded in human perceptual judgments. We show that these perceptual losses lead to representations are superior to other methods, both with respect to reconstructing given images, and generating novel ones. This superiority is demonstrated both in quantitative studies and human judgements ... We (also) demonstrate that perceptual losses yield a convincing win when applied to a state-of-the-art architecture for single image super-resolution.\n",
    "\n",
    "As observed in the deterministic case, MS-SSIM is better at capturing fine details than either MSE or MAE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCHS      = 12501\n",
    "BATCH_SIZE  = 4\n",
    "\n",
    "MODEL_STEPS = 50\n",
    "GIF_STEPS   = 10\n",
    "\n",
    "SAMPLES =  np.random.permutation(FLAT)[:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gifit(epoch=None):\n",
    "    if (epoch % GIF_STEPS == 0):\n",
    "        print('saving gif ...')\n",
    "        z,y,yc,i,ic,s,sc = AUTOENCODER.predict_on_batch(SAMPLES)\n",
    "        img = np.clip(127.5*(s+1).reshape((-1, SCALE_FACTOR*SIZE, SCALE_FACTOR*SIZE, CHANNELS)), 0, 255)\n",
    "        RECONS.append(utils.montage(img).astype(np.uint8))\n",
    "        \n",
    "def saveit(epoch=None):\n",
    "    if ((epoch > 0) and (epoch % MODEL_STEPS == 0)):\n",
    "        print('saving model ...')\n",
    "        AUTOENCODER.save(MODEL_NAME+'-autoencoder-model.h5')\n",
    "        ENCODER.save(MODEL_NAME+'-encoder-model.h5')\n",
    "        DECODER.save(MODEL_NAME+'-generator-model.h5')\n",
    "        SUPER.save(MODEL_NAME+'-super-model.h5')\n",
    "        SUPERSIZER.save(MODEL_NAME+'-supersizer-model.h5')\n",
    "        print('done')\n",
    "       \n",
    "        \n",
    "# callbacks\n",
    "giffer = LambdaCallback(on_epoch_end=lambda epoch, logs: gifit(epoch))\n",
    "saver = LambdaCallback(on_epoch_end=lambda epoch, logs: saveit(epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width =\"1024\" src=\"https://s3.amazonaws.com/neurokinetikz/Layer+1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Encoder/Decoder\n",
    "if (SIZE == 256):\n",
    "    FILTERS = [64,80,96,128,160,192]\n",
    "    \n",
    "elif (SIZE == 128):\n",
    "    FILTERS = [64,96,128,160,192]\n",
    "    \n",
    "elif (SIZE == 64):\n",
    "    FILTERS = [64,96,128,160]\n",
    "    \n",
    "elif (SIZE == 32):\n",
    "    FILTERS = [64,96,128]\n",
    "\n",
    "# Residuals\n",
    "R_LAYERS  = 16\n",
    "R_FILTERS = 64\n",
    "R_SCALING = 0.01\n",
    "\n",
    "# Attention Modules\n",
    "R_ATTENTION = True\n",
    "R_REDUCTION = 16\n",
    "R_DILATION = 4\n",
    "\n",
    "# Latent dimension size\n",
    "LATENT_DIM = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 196608)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 256, 256, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 256, 256, 64) 1792        reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_1 (GroupNor (None, 256, 256, 64) 128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 256, 256, 64) 0           group_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 256, 256, 64) 36928       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_2 (GroupNor (None, 256, 256, 64) 128         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 256, 256, 64) 0           group_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 128, 128, 64) 0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 128, 128, 80) 46160       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_3 (GroupNor (None, 128, 128, 80) 160         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 128, 128, 80) 0           group_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 128, 128, 80) 57680       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_4 (GroupNor (None, 128, 128, 80) 160         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 128, 128, 80) 0           group_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 64, 64, 80)   0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 64, 64, 96)   69216       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_5 (GroupNor (None, 64, 64, 96)   192         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 64, 64, 96)   0           group_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 64, 64, 96)   83040       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_6 (GroupNor (None, 64, 64, 96)   192         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 64, 64, 96)   0           group_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 32, 32, 96)   0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 128)  110720      max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_7 (GroupNor (None, 32, 32, 128)  256         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 128)  0           group_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 128)  147584      activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_8 (GroupNor (None, 32, 32, 128)  256         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 32, 128)  0           group_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 16, 16, 128)  0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 160)  184480      max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_9 (GroupNor (None, 16, 16, 160)  320         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 160)  0           group_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 160)  230560      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_10 (GroupNo (None, 16, 16, 160)  320         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 160)  0           group_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 8, 8, 160)    0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 8, 8, 192)    276672      max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_11 (GroupNo (None, 8, 8, 192)    384         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 8, 8, 192)    0           group_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 8, 8, 192)    331968      activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_12 (GroupNo (None, 8, 8, 192)    384         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 8, 8, 192)    0           group_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 4, 4, 192)    0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 3072)         0           max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Dense)                 (None, 1024)         3146752     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "generator (Dense)               (None, 3072)         3148800     encoder[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 4, 4, 192)    0           generator[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 8, 8, 192)    0           reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 8, 8, 192)    331968      up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_13 (GroupNo (None, 8, 8, 192)    384         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 8, 8, 192)    0           group_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 8, 8, 192)    331968      activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_14 (GroupNo (None, 8, 8, 192)    384         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 8, 8, 192)    0           group_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 16, 16, 192)  0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 16, 16, 160)  276640      up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_15 (GroupNo (None, 16, 16, 160)  320         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 16, 16, 160)  0           group_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 16, 16, 160)  230560      activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_16 (GroupNo (None, 16, 16, 160)  320         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 16, 16, 160)  0           group_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 32, 32, 160)  0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 32, 32, 128)  184448      up_sampling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_17 (GroupNo (None, 32, 32, 128)  256         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 32, 32, 128)  0           group_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 32, 32, 128)  147584      activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_18 (GroupNo (None, 32, 32, 128)  256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 32, 32, 128)  0           group_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2D)  (None, 64, 64, 128)  0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 64, 64, 96)   110688      up_sampling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_19 (GroupNo (None, 64, 64, 96)   192         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 64, 64, 96)   0           group_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 64, 64, 96)   83040       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_20 (GroupNo (None, 64, 64, 96)   192         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 64, 64, 96)   0           group_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2D)  (None, 128, 128, 96) 0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 128, 128, 80) 69200       up_sampling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_21 (GroupNo (None, 128, 128, 80) 160         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 128, 128, 80) 0           group_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 128, 128, 80) 57680       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_22 (GroupNo (None, 128, 128, 80) 160         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 128, 128, 80) 0           group_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_6 (UpSampling2D)  (None, 256, 256, 80) 0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 256, 256, 64) 46144       up_sampling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_23 (GroupNo (None, 256, 256, 64) 128         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 256, 256, 64) 0           group_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 256, 256, 64) 36928       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_24 (GroupNo (None, 256, 256, 64) 128         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 256, 256, 64) 0           group_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 256, 256, 3)  195         activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_dssim (Activation)      (None, 256, 256, 3)  0           conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 256, 256, 64) 1792        decoder_dssim[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 256, 256, 64) 36928       conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 256, 256, 64) 0           conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 256, 256, 64) 36928       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 256, 256, 64) 0           conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_variance_pooling2d_1 (Gl (None, 64)           0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 256, 256, 4)  260         lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 1, 1, 64)     0           global_variance_pooling2d_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 256, 256, 4)  148         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 1, 1, 4)      260         reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 256, 256, 4)  148         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 1, 1, 4)      0           conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 256, 256, 1)  5           conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 1, 1, 64)     320         activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_25 (GroupNo (None, 256, 256, 1)  2           conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 256, 256, 64) 0           conv2d_31[0][0]                  \n",
      "                                                                 group_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 256, 256, 64) 0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 256, 256, 64) 0           activation_27[0][0]              \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 256, 256, 64) 0           multiply_1[0][0]                 \n",
      "                                                                 conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 256, 256, 64) 36928       add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 256, 256, 64) 0           conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 256, 256, 64) 36928       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 256, 256, 64) 0           conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_variance_pooling2d_2 (Gl (None, 64)           0           lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 256, 256, 4)  260         lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 1, 1, 64)     0           global_variance_pooling2d_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 256, 256, 4)  148         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 1, 1, 4)      260         reshape_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 256, 256, 4)  148         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 1, 1, 4)      0           conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 256, 256, 1)  5           conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 1, 1, 64)     320         activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_26 (GroupNo (None, 256, 256, 1)  2           conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 256, 256, 64) 0           conv2d_39[0][0]                  \n",
      "                                                                 group_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 256, 256, 64) 0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 256, 256, 64) 0           activation_30[0][0]              \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 256, 256, 64) 0           multiply_2[0][0]                 \n",
      "                                                                 add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 256, 256, 64) 36928       add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 256, 256, 64) 0           conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 256, 256, 64) 36928       activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 256, 256, 64) 0           conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_variance_pooling2d_3 (Gl (None, 64)           0           lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 256, 256, 4)  260         lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 1, 1, 64)     0           global_variance_pooling2d_3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 256, 256, 4)  148         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 1, 1, 4)      260         reshape_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 256, 256, 4)  148         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 1, 1, 4)      0           conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 256, 256, 1)  5           conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 1, 1, 64)     320         activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_27 (GroupNo (None, 256, 256, 1)  2           conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 256, 256, 64) 0           conv2d_47[0][0]                  \n",
      "                                                                 group_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 256, 256, 64) 0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_3 (Multiply)           (None, 256, 256, 64) 0           activation_33[0][0]              \n",
      "                                                                 lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 256, 256, 64) 0           multiply_3[0][0]                 \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 256, 256, 64) 36928       add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 256, 256, 64) 0           conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 256, 256, 64) 36928       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 256, 256, 64) 0           conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_variance_pooling2d_4 (Gl (None, 64)           0           lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 256, 256, 4)  260         lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, 1, 1, 64)     0           global_variance_pooling2d_4[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 256, 256, 4)  148         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 1, 1, 4)      260         reshape_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 256, 256, 4)  148         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1, 1, 4)      0           conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 256, 256, 1)  5           conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 1, 1, 64)     320         activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_28 (GroupNo (None, 256, 256, 1)  2           conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 256, 256, 64) 0           conv2d_55[0][0]                  \n",
      "                                                                 group_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 256, 256, 64) 0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_4 (Multiply)           (None, 256, 256, 64) 0           activation_36[0][0]              \n",
      "                                                                 lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 256, 256, 64) 0           multiply_4[0][0]                 \n",
      "                                                                 add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 256, 256, 64) 36928       add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 256, 256, 64) 0           conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 256, 256, 64) 36928       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 256, 256, 64) 0           conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_variance_pooling2d_5 (Gl (None, 64)           0           lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 256, 256, 4)  260         lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_7 (Reshape)             (None, 1, 1, 64)     0           global_variance_pooling2d_5[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 256, 256, 4)  148         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 1, 1, 4)      260         reshape_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 256, 256, 4)  148         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1, 1, 4)      0           conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 256, 256, 1)  5           conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 1, 1, 64)     320         activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_29 (GroupNo (None, 256, 256, 1)  2           conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 256, 256, 64) 0           conv2d_63[0][0]                  \n",
      "                                                                 group_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 256, 256, 64) 0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_5 (Multiply)           (None, 256, 256, 64) 0           activation_39[0][0]              \n",
      "                                                                 lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 256, 256, 64) 0           multiply_5[0][0]                 \n",
      "                                                                 add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 256, 256, 64) 36928       add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 256, 256, 64) 0           conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 256, 256, 64) 36928       activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 256, 256, 64) 0           conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_variance_pooling2d_6 (Gl (None, 64)           0           lambda_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 256, 256, 4)  260         lambda_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_8 (Reshape)             (None, 1, 1, 64)     0           global_variance_pooling2d_6[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 256, 256, 4)  148         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 1, 1, 4)      260         reshape_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 256, 256, 4)  148         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 1, 1, 4)      0           conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 256, 256, 1)  5           conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 1, 1, 64)     320         activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_30 (GroupNo (None, 256, 256, 1)  2           conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 256, 256, 64) 0           conv2d_71[0][0]                  \n",
      "                                                                 group_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 256, 256, 64) 0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_6 (Multiply)           (None, 256, 256, 64) 0           activation_42[0][0]              \n",
      "                                                                 lambda_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 256, 256, 64) 0           multiply_6[0][0]                 \n",
      "                                                                 add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 256, 256, 64) 36928       add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 256, 256, 64) 0           conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 256, 256, 64) 36928       activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 256, 256, 64) 0           conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_variance_pooling2d_7 (Gl (None, 64)           0           lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 256, 256, 4)  260         lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_9 (Reshape)             (None, 1, 1, 64)     0           global_variance_pooling2d_7[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 256, 256, 4)  148         conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 1, 1, 4)      260         reshape_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 256, 256, 4)  148         conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 1, 1, 4)      0           conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 256, 256, 1)  5           conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 1, 1, 64)     320         activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_31 (GroupNo (None, 256, 256, 1)  2           conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 256, 256, 64) 0           conv2d_79[0][0]                  \n",
      "                                                                 group_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 256, 256, 64) 0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_7 (Multiply)           (None, 256, 256, 64) 0           activation_45[0][0]              \n",
      "                                                                 lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 256, 256, 64) 0           multiply_7[0][0]                 \n",
      "                                                                 add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 256, 256, 64) 36928       add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 256, 256, 64) 0           conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 256, 256, 64) 36928       activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 256, 256, 64) 0           conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_variance_pooling2d_8 (Gl (None, 64)           0           lambda_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 256, 256, 4)  260         lambda_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_10 (Reshape)            (None, 1, 1, 64)     0           global_variance_pooling2d_8[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 256, 256, 4)  148         conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 1, 1, 4)      260         reshape_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 256, 256, 4)  148         conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1, 1, 4)      0           conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 256, 256, 1)  5           conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 1, 1, 64)     320         activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_32 (GroupNo (None, 256, 256, 1)  2           conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 256, 256, 64) 0           conv2d_87[0][0]                  \n",
      "                                                                 group_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 256, 256, 64) 0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_8 (Multiply)           (None, 256, 256, 64) 0           activation_48[0][0]              \n",
      "                                                                 lambda_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 256, 256, 64) 0           multiply_8[0][0]                 \n",
      "                                                                 add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 256, 256, 64) 36928       add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 256, 256, 64) 0           conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 256, 256, 64) 36928       activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 256, 256, 64) 0           conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_variance_pooling2d_9 (Gl (None, 64)           0           lambda_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 256, 256, 4)  260         lambda_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_11 (Reshape)            (None, 1, 1, 64)     0           global_variance_pooling2d_9[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 256, 256, 4)  148         conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 1, 1, 4)      260         reshape_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 256, 256, 4)  148         conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 1, 1, 4)      0           conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 256, 256, 1)  5           conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 1, 1, 64)     320         activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_33 (GroupNo (None, 256, 256, 1)  2           conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 256, 256, 64) 0           conv2d_95[0][0]                  \n",
      "                                                                 group_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 256, 256, 64) 0           add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_9 (Multiply)           (None, 256, 256, 64) 0           activation_51[0][0]              \n",
      "                                                                 lambda_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 256, 256, 64) 0           multiply_9[0][0]                 \n",
      "                                                                 add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, 256, 256, 64) 36928       add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 256, 256, 64) 0           conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, 256, 256, 64) 36928       activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 256, 256, 64) 0           conv2d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_variance_pooling2d_10 (G (None, 64)           0           lambda_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 256, 256, 4)  260         lambda_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_12 (Reshape)            (None, 1, 1, 64)     0           global_variance_pooling2d_10[0][0\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, 256, 256, 4)  148         conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, 1, 1, 4)      260         reshape_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (None, 256, 256, 4)  148         conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1, 1, 4)      0           conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_107 (Conv2D)             (None, 256, 256, 1)  5           conv2d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, 1, 1, 64)     320         activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_34 (GroupNo (None, 256, 256, 1)  2           conv2d_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 256, 256, 64) 0           conv2d_103[0][0]                 \n",
      "                                                                 group_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 256, 256, 64) 0           add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_10 (Multiply)          (None, 256, 256, 64) 0           activation_54[0][0]              \n",
      "                                                                 lambda_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 256, 256, 64) 0           multiply_10[0][0]                \n",
      "                                                                 add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_108 (Conv2D)             (None, 256, 256, 64) 36928       add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 256, 256, 64) 0           conv2d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_109 (Conv2D)             (None, 256, 256, 64) 36928       activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 256, 256, 64) 0           conv2d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_variance_pooling2d_11 (G (None, 64)           0           lambda_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_112 (Conv2D)             (None, 256, 256, 4)  260         lambda_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_13 (Reshape)            (None, 1, 1, 64)     0           global_variance_pooling2d_11[0][0\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_113 (Conv2D)             (None, 256, 256, 4)  148         conv2d_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_110 (Conv2D)             (None, 1, 1, 4)      260         reshape_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_114 (Conv2D)             (None, 256, 256, 4)  148         conv2d_113[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 1, 1, 4)      0           conv2d_110[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_115 (Conv2D)             (None, 256, 256, 1)  5           conv2d_114[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_111 (Conv2D)             (None, 1, 1, 64)     320         activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_35 (GroupNo (None, 256, 256, 1)  2           conv2d_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 256, 256, 64) 0           conv2d_111[0][0]                 \n",
      "                                                                 group_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 256, 256, 64) 0           add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_11 (Multiply)          (None, 256, 256, 64) 0           activation_57[0][0]              \n",
      "                                                                 lambda_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 256, 256, 64) 0           multiply_11[0][0]                \n",
      "                                                                 add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_116 (Conv2D)             (None, 256, 256, 64) 36928       add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 256, 256, 64) 0           conv2d_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_117 (Conv2D)             (None, 256, 256, 64) 36928       activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)              (None, 256, 256, 64) 0           conv2d_117[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_variance_pooling2d_12 (G (None, 64)           0           lambda_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_120 (Conv2D)             (None, 256, 256, 4)  260         lambda_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_14 (Reshape)            (None, 1, 1, 64)     0           global_variance_pooling2d_12[0][0\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_121 (Conv2D)             (None, 256, 256, 4)  148         conv2d_120[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_118 (Conv2D)             (None, 1, 1, 4)      260         reshape_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_122 (Conv2D)             (None, 256, 256, 4)  148         conv2d_121[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 1, 1, 4)      0           conv2d_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_123 (Conv2D)             (None, 256, 256, 1)  5           conv2d_122[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_119 (Conv2D)             (None, 1, 1, 64)     320         activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_36 (GroupNo (None, 256, 256, 1)  2           conv2d_123[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, 256, 256, 64) 0           conv2d_119[0][0]                 \n",
      "                                                                 group_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 256, 256, 64) 0           add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_12 (Multiply)          (None, 256, 256, 64) 0           activation_60[0][0]              \n",
      "                                                                 lambda_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (None, 256, 256, 64) 0           multiply_12[0][0]                \n",
      "                                                                 add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_124 (Conv2D)             (None, 256, 256, 64) 36928       add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 256, 256, 64) 0           conv2d_124[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_125 (Conv2D)             (None, 256, 256, 64) 36928       activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_13 (Lambda)              (None, 256, 256, 64) 0           conv2d_125[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_variance_pooling2d_13 (G (None, 64)           0           lambda_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_128 (Conv2D)             (None, 256, 256, 4)  260         lambda_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_15 (Reshape)            (None, 1, 1, 64)     0           global_variance_pooling2d_13[0][0\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_129 (Conv2D)             (None, 256, 256, 4)  148         conv2d_128[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_126 (Conv2D)             (None, 1, 1, 4)      260         reshape_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_130 (Conv2D)             (None, 256, 256, 4)  148         conv2d_129[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 1, 1, 4)      0           conv2d_126[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_131 (Conv2D)             (None, 256, 256, 1)  5           conv2d_130[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_127 (Conv2D)             (None, 1, 1, 64)     320         activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_37 (GroupNo (None, 256, 256, 1)  2           conv2d_131[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 256, 256, 64) 0           conv2d_127[0][0]                 \n",
      "                                                                 group_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 256, 256, 64) 0           add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_13 (Multiply)          (None, 256, 256, 64) 0           activation_63[0][0]              \n",
      "                                                                 lambda_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 256, 256, 64) 0           multiply_13[0][0]                \n",
      "                                                                 add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_132 (Conv2D)             (None, 256, 256, 64) 36928       add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 256, 256, 64) 0           conv2d_132[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_133 (Conv2D)             (None, 256, 256, 64) 36928       activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_14 (Lambda)              (None, 256, 256, 64) 0           conv2d_133[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_variance_pooling2d_14 (G (None, 64)           0           lambda_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_136 (Conv2D)             (None, 256, 256, 4)  260         lambda_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_16 (Reshape)            (None, 1, 1, 64)     0           global_variance_pooling2d_14[0][0\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_137 (Conv2D)             (None, 256, 256, 4)  148         conv2d_136[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_134 (Conv2D)             (None, 1, 1, 4)      260         reshape_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_138 (Conv2D)             (None, 256, 256, 4)  148         conv2d_137[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 1, 1, 4)      0           conv2d_134[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_139 (Conv2D)             (None, 256, 256, 1)  5           conv2d_138[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_135 (Conv2D)             (None, 1, 1, 64)     320         activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_38 (GroupNo (None, 256, 256, 1)  2           conv2d_139[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_27 (Add)                    (None, 256, 256, 64) 0           conv2d_135[0][0]                 \n",
      "                                                                 group_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 256, 256, 64) 0           add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_14 (Multiply)          (None, 256, 256, 64) 0           activation_66[0][0]              \n",
      "                                                                 lambda_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_28 (Add)                    (None, 256, 256, 64) 0           multiply_14[0][0]                \n",
      "                                                                 add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_140 (Conv2D)             (None, 256, 256, 64) 36928       add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 256, 256, 64) 0           conv2d_140[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_141 (Conv2D)             (None, 256, 256, 64) 36928       activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_15 (Lambda)              (None, 256, 256, 64) 0           conv2d_141[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_variance_pooling2d_15 (G (None, 64)           0           lambda_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_144 (Conv2D)             (None, 256, 256, 4)  260         lambda_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_17 (Reshape)            (None, 1, 1, 64)     0           global_variance_pooling2d_15[0][0\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_145 (Conv2D)             (None, 256, 256, 4)  148         conv2d_144[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_142 (Conv2D)             (None, 1, 1, 4)      260         reshape_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_146 (Conv2D)             (None, 256, 256, 4)  148         conv2d_145[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 1, 1, 4)      0           conv2d_142[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_147 (Conv2D)             (None, 256, 256, 1)  5           conv2d_146[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_143 (Conv2D)             (None, 1, 1, 64)     320         activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_39 (GroupNo (None, 256, 256, 1)  2           conv2d_147[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_29 (Add)                    (None, 256, 256, 64) 0           conv2d_143[0][0]                 \n",
      "                                                                 group_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 256, 256, 64) 0           add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_15 (Multiply)          (None, 256, 256, 64) 0           activation_69[0][0]              \n",
      "                                                                 lambda_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_30 (Add)                    (None, 256, 256, 64) 0           multiply_15[0][0]                \n",
      "                                                                 add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_148 (Conv2D)             (None, 256, 256, 64) 36928       add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 256, 256, 64) 0           conv2d_148[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_149 (Conv2D)             (None, 256, 256, 64) 36928       activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_16 (Lambda)              (None, 256, 256, 64) 0           conv2d_149[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_variance_pooling2d_16 (G (None, 64)           0           lambda_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_152 (Conv2D)             (None, 256, 256, 4)  260         lambda_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_18 (Reshape)            (None, 1, 1, 64)     0           global_variance_pooling2d_16[0][0\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_153 (Conv2D)             (None, 256, 256, 4)  148         conv2d_152[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_150 (Conv2D)             (None, 1, 1, 4)      260         reshape_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_154 (Conv2D)             (None, 256, 256, 4)  148         conv2d_153[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 1, 1, 4)      0           conv2d_150[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_155 (Conv2D)             (None, 256, 256, 1)  5           conv2d_154[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_151 (Conv2D)             (None, 1, 1, 64)     320         activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "group_normalization_40 (GroupNo (None, 256, 256, 1)  2           conv2d_155[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_31 (Add)                    (None, 256, 256, 64) 0           conv2d_151[0][0]                 \n",
      "                                                                 group_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 256, 256, 64) 0           add_31[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_16 (Multiply)          (None, 256, 256, 64) 0           activation_72[0][0]              \n",
      "                                                                 lambda_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_32 (Add)                    (None, 256, 256, 64) 0           multiply_16[0][0]                \n",
      "                                                                 add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_156 (Conv2D)             (None, 256, 256, 3)  195         add_32[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 256, 256, 3)  12          decoder_dssim[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_33 (Add)                    (None, 256, 256, 3)  0           conv2d_156[0][0]                 \n",
      "                                                                 conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "refiner_dssim (Activation)      (None, 256, 256, 3)  0           add_33[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_157 (Conv2D)             (None, 256, 256, 256 7168        refiner_dssim[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 256, 256, 256 0           conv2d_157[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "sub_pixel_upscaling_1 (SubPixel (None, 512, 512, 64) 0           activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_158 (Conv2D)             (None, 512, 512, 256 147712      sub_pixel_upscaling_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 512, 512, 256 0           conv2d_158[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_159 (Conv2D)             (None, 512, 512, 3)  771         activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "super_dssim (Activation)        (None, 512, 512, 3)  0           conv2d_159[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Flatten)               (None, 196608)       0           decoder_dssim[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "refiner (Flatten)               (None, 196608)       0           refiner_dssim[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "super (Flatten)                 (None, 786432)       0           super_dssim[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 11,142,789\n",
      "Trainable params: 11,142,789\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# input\n",
    "X = Input(shape=(FEATURES,))\n",
    "\n",
    "# encode\n",
    "Z, shape = encode(X)\n",
    "\n",
    "# decoder input\n",
    "Z_G = Input(shape=(LATENT_DIM,))\n",
    "\n",
    "# decode\n",
    "Y, Y_G, Y_F = decode(Z,Z_G,shape)\n",
    "\n",
    "# refine\n",
    "IMG, IMG_G, IMG_F = refine(Y,Y_G)\n",
    "\n",
    "# supersizer input\n",
    "X_G = Input(shape=(SIZE,SIZE,CHANNELS))\n",
    "\n",
    "# supersize\n",
    "IMG_S, IMG_G_S, X_G_S, IMG_S_F = upsample(IMG,IMG_G,X_G)\n",
    "\n",
    "\n",
    "# model definitions\n",
    "ENCODER = Model(inputs=[X], outputs=[Z])\n",
    "DECODER = Model(inputs=[Z_G], outputs=[IMG_G])\n",
    "SUPER = Model(inputs=[Z_G], outputs=[IMG_G_S])\n",
    "SUPERSIZER = Model(inputs=[X_G], outputs=[X_G_S])\n",
    "\n",
    "# define optimizer\n",
    "ADAM = optimizers.Adam(amsgrad=True)\n",
    "\n",
    "# compile models\n",
    "ENCODER.compile(optimizer=ADAM,loss='mse')\n",
    "DECODER.compile(optimizer=ADAM,loss='mae')\n",
    "SUPER.compile(optimizer=ADAM,loss='mae')\n",
    "SUPERSIZER.compile(optimizer=ADAM,loss='mae')\n",
    "\n",
    "\n",
    "# define autoencoder\n",
    "AUTOENCODER = Model(inputs=[X], outputs=[Z,Y,Y_F,IMG,IMG_F,IMG_S,IMG_S_F])\n",
    "\n",
    "# define losses\n",
    "losses = {'encoder':vae_loss,\n",
    "          'decoder':'mse',\n",
    "          'decoder_dssim':DSSIMObjective(),\n",
    "          'refiner':'mae',\n",
    "          'refiner_dssim':DSSIMObjective(),\n",
    "          'super':'mae',\n",
    "          'super_dssim':DSSIMObjective()}\n",
    "\n",
    "# compile model\n",
    "AUTOENCODER.compile(optimizer=ADAM,loss=losses)\n",
    "\n",
    "# print summary\n",
    "AUTOENCODER.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "RECONS = []\n",
    "\n",
    "# fit model\n",
    "AUTOENCODER.fit(x=FLAT,\n",
    "                y=[FLAT,IMGS,FLAT,IMGS,FLAT,IMGS_2X,FLAT_2X],\n",
    "                batch_size=BATCH_SIZE,\n",
    "                epochs=EPOCHS,\n",
    "                callbacks=[giffer,saver])\n",
    "\n",
    "# save training gif\n",
    "gif.build_gif(RECONS, saveto=MODEL_NAME+'-final'+ \"-\"+str(time.time())+'.gif')\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = 'roadtrip-64-1545070403.8379235'\n",
    "\n",
    "print('loading encoder ...', MODEL_NAME)\n",
    "ENCODER = load_model(MODEL_NAME+'-encoder-model.h5')\n",
    "\n",
    "print('loading decoder ...')\n",
    "DECODER = load_model(MODEL_NAME+'-generator-model.h5', custom_objects={'R_SCALING':R_SCALING,'GlobalVariancePooling2D':GlobalVariancePooling2D})\n",
    "\n",
    "print('loading supersizer ...')\n",
    "SUPERSIZER = load_model(MODEL_NAME+'-supersizer-model.h5')\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img width=600 src=\"https://s3.amazonaws.com/neurokinetikz/Screen+Shot+2018-11-13+at+12.17.10+PM.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reconstruct(index=0):\n",
    "    \n",
    "    # input\n",
    "    x = np.reshape(FLAT[index],(-1,FEATURES))\n",
    "    z = ENCODER.predict_on_batch(x)\n",
    "    \n",
    "    # output\n",
    "    img = np.reshape(DECODER.predict_on_batch(z),(-1,FEATURES))\n",
    "    img_s = np.reshape(SUPER.predict_on_batch(z),(-1,FEATURES*SCALE_FACTOR*SCALE_FACTOR))\n",
    "    \n",
    "    # reference\n",
    "    ref = IMGS[index]/2 + .5\n",
    "    ref_s = IMGS_2X[index]/2 + .5\n",
    "    \n",
    "    # denormalize\n",
    "    img = np.reshape(img/2 + .5,(SIZE,SIZE,CHANNELS))\n",
    "    img_s= np.reshape(img_s/2 + .5,(SCALE_FACTOR*SIZE,SCALE_FACTOR*SIZE,CHANNELS))\n",
    "    \n",
    "    # print scores\n",
    "    print(\"PSNR: %.3f %.3f <> MS-SSIM: %.3f %.3f\" % ((utils.psnr(ref,img)),\n",
    "                                                     (utils.psnr(ref_s,img_s)),\n",
    "                                           (utils.MultiScaleSSIM(np.reshape(ref,(1,SIZE,SIZE,CHANNELS)),\n",
    "                                                                 np.reshape(img,(1,SIZE,SIZE,CHANNELS)),\n",
    "                                                                 max_val=1.)),\n",
    "                                            (utils.MultiScaleSSIM(np.reshape(ref_s,(1,SCALE_FACTOR*SIZE,SCALE_FACTOR*SIZE,CHANNELS)),\n",
    "                                                                 np.reshape(img_s,(1,SCALE_FACTOR*SIZE,SCALE_FACTOR*SIZE,CHANNELS)),\n",
    "                                                                 max_val=1.))\n",
    "                                               ))\n",
    "    \n",
    "    # show images\n",
    "    utils.showImagesHorizontally(images=[ref,img])\n",
    "    utils.showImagesHorizontally(images=[ref_s,img_s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reconstruct(random.randint(0,TOTAL_BATCH-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent  Animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.youtube.com/watch?v=grEi3uRlSb4\n",
    "<table><tr>\n",
    "<td><img src=\"https://s3.amazonaws.com/neurokinetikz/1542113809.6163912-109.gif\"></td>\n",
    "<td><img src=\"https://s3.amazonaws.com/neurokinetikz/1542113809.6163912-100.gif\"></td>\n",
    "<td><img src=\"https://s3.amazonaws.com/neurokinetikz/1542113809.6163912-080.gif\"></td>\n",
    "</tr></table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_latents(n_imgs=3,steps=30):\n",
    "    rimgs = np.random.permutation(FLAT)[:n_imgs]\n",
    "    rimgs = np.append(rimgs, [rimgs[0]],axis=0)\n",
    "    latent_animation(rimgs,steps,filename=str(time.time()))\n",
    "\n",
    "def latent_animation(imgs=None,steps=None,filename=\"latent-animation\"):\n",
    "    animate(generate(get_latents(imgs,steps),filename),filename)\n",
    "    \n",
    "def get_latents(imgs,steps):\n",
    "    # get latent encodings for images\n",
    "#     print('getting latent vectors ...')\n",
    "    latents = []\n",
    "    for index,img in enumerate(imgs):\n",
    "        img = np.reshape(img,(-1,FEATURES))\n",
    "        latent = ENCODER.predict_on_batch(img)\n",
    "        latents.append(latent)\n",
    "\n",
    "    # calculate latent path\n",
    "#     print('calculating latent path ...')\n",
    "    latent_path = []\n",
    "    for i in range(len(latents)-1):\n",
    "        # get latent vectors\n",
    "        l1 = latents[i] ; l2 = latents[i+1]\n",
    "\n",
    "        # calculate latent distance\n",
    "        image_distance = l2 - l1\n",
    "\n",
    "        # create the latent path\n",
    "        for j in range(steps):\n",
    "            latent_path.append(l1 + j*image_distance/steps)\n",
    "        latent_path.append(l2)\n",
    "    \n",
    "    return latent_path\n",
    "       \n",
    "    \n",
    "def generate(latent_path,filename=None):\n",
    "     # reconstruct images along the path\n",
    "#     print('reconstructing latent paths... ')\n",
    "    latent_path = np.reshape(latent_path,(-1,LATENT_DIM))\n",
    "    \n",
    "#     print('decoding ...')\n",
    "    recons = DECODER.predict_on_batch(latent_path)\n",
    "    \n",
    "    if(filename != None):\n",
    "        print('saving decoder gif')\n",
    "        build_gif(np.asarray(recons),SIZE,filename)\n",
    "    \n",
    "    return recons\n",
    "    \n",
    "def animate(recons,filename):\n",
    "    print('supersizing ...')\n",
    "    chunks = SUPERSIZER.predict_on_batch(recons[:10])\n",
    "    for i in range(10,len(recons)-10,10):\n",
    "        s2 = SUPERSIZER.predict_on_batch(recons[i:i+10])\n",
    "        chunks = np.concatenate([chunks,s2])\n",
    "    \n",
    "    print('saving supersizer gif')\n",
    "    build_gif(chunks,SIZE*SCALE_FACTOR,filename+\"-\"+str(SCALE_FACTOR)+\"x\")\n",
    "   \n",
    "    # done\n",
    "    print(filename)\n",
    "    \n",
    "def build_gif(recons,size,filename='latent-animation'):\n",
    "    final = np.clip((127.5*(recons+1)).reshape((-1,size,size,CHANNELS)),0,255)\n",
    "    gif.build_gif([utils.montage([r]).astype(np.uint8) for r in final], saveto=filename+\".gif\",dpi=72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "LATENT_DIM=1024\n",
    "for i in range(100):\n",
    "    random_latents(3,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grid=[]\n",
    "GRID = 100\n",
    "N_IMGS = 4\n",
    "STEPS = 50\n",
    "\n",
    "for i in range(GRID):\n",
    "    imgs = np.random.permutation(FLAT)[:N_IMGS]\n",
    "    imgs = np.append(imgs, [imgs[0]],axis=0)\n",
    "    \n",
    "    latent_path = get_latents(imgs,STEPS)\n",
    "    recons = generate(latent_path)\n",
    "    final = np.clip((127.5*(recons+1)).reshape((-1,SIZE,SIZE,CHANNELS)),0,255)\n",
    "    grid.append(final)\n",
    "    \n",
    "print(np.asarray(grid).shape)\n",
    "\n",
    "\n",
    "rs = []\n",
    "\n",
    "for img in grid:\n",
    "    rs.append([frame for frame in img])\n",
    "    \n",
    "rs = np.moveaxis(grid,1,0)\n",
    "\n",
    "print(np.asarray(rs).shape)\n",
    "    \n",
    "gif.build_gif([utils.montage(r).astype(np.uint8) for r in rs], saveto=str(time.time())+\".gif\",dpi=72)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imgs =  np.random.permutation(FLAT)\n",
    "t = str(time.time())\n",
    "for i in range(TOTAL_BATCH):\n",
    "    print(i)\n",
    "    latent_animation([imgs[i],imgs[i+1]],25,filename=t+'-'+ ('%03d' % i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model & Continue Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('load model ...')\n",
    "AUTOENCODER = load_model(MODEL_NAME+'-autoencoder-model.h5',\n",
    "                         custom_objects={'vae_loss': vae_loss, \n",
    "                                         'R_SCALING':0.1, \n",
    "                                         'DSSIMObjective':DSSIMObjective()})\n",
    "\n",
    "# define encoder\n",
    "ENCODER = Model(inputs=[AUTOENCODER.input], outputs=[AUTOENCODER.get_layer(\"encoder\").output])\n",
    "\n",
    "# define generator\n",
    "Z = Input(shape=(LATENT_DIM,))\n",
    "GENERATOR = Model(inputs=[Z], outputs=[AUTOENCODER.get_layer(\"generator\")(Z)])\n",
    "\n",
    "ENCODER.compile(optimizer=ADAM,loss='mse')\n",
    "GENERATOR.compile(optimizer=ADAM,loss='mse')\n",
    "\n",
    "print('resume training ...')\n",
    "AUTOENCODER.fit(x=FLAT,y=[FLAT,IMGS,FLAT,IMGS,FLAT],batch_size=BATCH_SIZE,epochs=EPOCHS,callbacks=[giffer,saver])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
